{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kili Tutorial: How to leverage Counterfactually augmented data to have a more robust model\n",
    "\n",
    "This recipe is inspired by the paper *Learning the Difference that Makes a Difference with Counterfactually-Augmented Data*, that you can find here on [arXiv](https://arxiv.org/abs/1909.12434)\n",
    "\n",
    "In this study, the authors point out the difficulty for Machine Learning models to generalize the classification rules learned, because their decision rules, described as 'spurious patterns', often miss the key elements that affects most the class of a text. They thus decided to delete what can be considered as a confusion factor, by changing the label of an asset at the same time as changing the minimum amount of words so those **key-words** would be much easier for the model to spot.\n",
    "\n",
    "We'll see in this tutorial :\n",
    "1. How to create a project in Kili, both for [IMDB](##Data-Augmentation-on-IMDB-dataset) and [SNLI](##Data-Augmentation-on-SNLI-dataset) datasets, to reproduce such a data-augmentation task, in order to improve our model, and decrease its variance when used in production with unseen data.\n",
    "2. We'll also try to [reproduce the results of the paper](##Reproducing-the-results), using similar models, to show how such a technique can be of key interest while working on a text-classification task.\n",
    "We'll use the data of the study, both IMDB and Stanford NLI, publicly available [here](https://github.com/acmi-lab/counterfactually-augmented-data).\n",
    "\n",
    "Additionally, for an overview of Kili, visit the [website](https://kili-technology.com), you can also check out the Kili [documentation](https://cloud.kili-technology.com/docs), or some other recipes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![data augmentation](https://raw.githubusercontent.com/acmi-lab/counterfactually-augmented-data/master/data_collection_pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentication\n",
    "import os\n",
    "\n",
    "# !pip install kili # uncomment if you don't have kili installed already\n",
    "from kili.client import Kili\n",
    "\n",
    "api_endpoint = os.getenv('KILI_API_ENDPOINT') \n",
    "# If you use Kili SaaS, use the url 'https://cloud.kili-technology.com/api/label/v2/graphql'\n",
    "\n",
    "kili = Kili(api_endpoint=api_endpoint)\n",
    "user_id = kili.auth.user_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation on IMDB dataset\n",
    "\n",
    "The data consists in reviews of films, that are classified as positives or negatives. State-of-the-art models performance is often measured against this dataset, making it a reference. \n",
    "\n",
    "This is how our task would look like on Kili, into 2 different projects for each task, from Positive to Negative or Negative to Positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "taskname = \"NEW_REVIEW\"\n",
    "project_imdb_negative_to_positive = {\n",
    "'title': 'Counterfactual data-augmentation - Negative to Positive',\n",
    "'description': 'IMDB Sentiment Analysis',\n",
    "'instructions': 'https://docs.google.com/document/d/1zhNaQrncBKc3aPKcnNa_mNpXlria28Ij7bfgUvJbyfw/edit?usp=sharing',\n",
    "'input_type': 'TEXT',\n",
    "'json_interface':{\n",
    "    \"filetype\": \"TEXT\",\n",
    "    \"jobs\": {\n",
    "        taskname : {\n",
    "            \"mlTask\": \"TRANSCRIPTION\",\n",
    "            \"content\": {\n",
    "                \"input\": None\n",
    "            },\n",
    "            \"required\": 1,\n",
    "            \"isChild\": False,\n",
    "            \"instruction\": \"Write here the new review modified to be POSITIVE. Please refer to the instructions above before starting\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "}\n",
    "project_imdb_positive_to_negative = {\n",
    "'title': 'Counterfactual data-augmentation - Positive to Negative',\n",
    "'description': 'IMDB Sentiment Analysis',\n",
    "'instructions': 'https://docs.google.com/document/d/1zhNaQrncBKc3aPKcnNa_mNpXlria28Ij7bfgUvJbyfw/edit?usp=sharing',\n",
    "'input_type': 'TEXT',\n",
    "'json_interface':{\n",
    "    \"jobs\": {\n",
    "        taskname : {\n",
    "            \"mlTask\": \"TRANSCRIPTION\",\n",
    "            \"content\": {\n",
    "                \"input\": None\n",
    "            },\n",
    "            \"required\": 1,\n",
    "            \"isChild\": False,\n",
    "            \"instruction\": \"Write here the new review modified to be NEGATIVE. Please refer to the instructions above before starting\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for project_imdb in [project_imdb_positive_to_negative,project_imdb_negative_to_positive] :\n",
    "    project_imdb['id'] = kili.create_project(title=project_imdb['title'],\n",
    "                                                   instructions=project_imdb['instructions'],\n",
    "                                                   description=project_imdb['description'],\n",
    "                                                   input_type=project_imdb['input_type'],\n",
    "                                                   json_interface=project_imdb['json_interface'])['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll just create some useful functions for an improved readability :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_assets(dataframe, intro, objective, instructions, truth_label, target_label) :\n",
    "    return((intro + dataframe[truth_label] + objective + dataframe[target_label] + instructions + dataframe['Text']).tolist())\n",
    "\n",
    "def create_json_responses(taskname,df,field=\"Text\") :\n",
    "    return( [{taskname: { \"text\": df[field].iloc[k] }\n",
    "          } for k in range(df.shape[0]) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data into Kili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "datasets = ['dev','train','test']\n",
    "\n",
    "for dataset in datasets :\n",
    "    url = f'https://raw.githubusercontent.com/acmi-lab/counterfactually-augmented-data/master/sentiment/combined/paired/{dataset}_paired.tsv'\n",
    "    df = pd.read_csv(url, error_bad_lines=False, sep='\\t')\n",
    "    df = df[df.index%2 == 0] # keep only the original reviews as assets\n",
    "    \n",
    "    for review_type,project_imdb in zip(['Positive','Negative'],[project_imdb_positive_to_negative,project_imdb_negative_to_positive]) :\n",
    "        dataframe = df[df['Sentiment']==review_type]\n",
    "        reviews_to_import = dataframe['Text'].tolist()\n",
    "        external_id_array = ('IMDB ' + review_type +' review ' + dataset + dataframe['batch_id'].astype('str')).tolist()\n",
    "    \n",
    "        kili.append_many_to_dataset(\n",
    "            project_id=project_imdb['id'],\n",
    "            content_array=reviews_to_import,\n",
    "            external_id_array=external_id_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the labels into Kili \n",
    "We will fill-in with the results of the study, as if they were predictions. In a real annotation project, we could fill in with the sentences as well so the labeler just has to write the changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'results-arxiv:1909.12434'\n",
    "\n",
    "for dataset in datasets :\n",
    "    url = f'https://raw.githubusercontent.com/acmi-lab/counterfactually-augmented-data/master/sentiment/combined/paired/{dataset}_paired.tsv'\n",
    "    df = pd.read_csv(url, error_bad_lines=False, sep='\\t')\n",
    "    df = df[df.index%2 == 1] # keep only the modified reviews as predictions\n",
    "    \n",
    "    for review_type,project_imdb in zip(['Positive','Negative'],[project_imdb_positive_to_negative,project_imdb_negative_to_positive]) :\n",
    "        dataframe = df[df['Sentiment']!=review_type]\n",
    "\n",
    "        external_id_array = ('IMDB ' + review_type +' review ' + dataset + dataframe['batch_id'].astype('str')).tolist()\n",
    "        json_response_array = create_json_responses(taskname,dataframe)\n",
    "    \n",
    "        kili.create_predictions(project_id=project_imdb['id'],\n",
    "            external_id_array=external_id_array,\n",
    "            model_name_array=[model_name]*len(external_id_array),\n",
    "            json_response_array=json_response_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how our interface looks in the end, allowing to quickly perform the task at hand\n",
    "\n",
    "![IMDB](./img/imdb_review.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation on SNLI dataset\n",
    "\n",
    "The data consists in a 3-class dataset, where, provided with two phrases, a premise and an hypothesis, the machine-learning task is to find the correct relation between those two sentences, that can be either entailment, contradiction or neutral.\n",
    "\n",
    "Here is an example of a premise, and three sentences that could be the hypothesis for the three categories :\n",
    "![examples](https://licor.me/post/img/robust-nlu/SNLI_annotation.png)\n",
    "\n",
    "This is how our task would look like on Kili, this time keeping it as a single project. To do so, we strongly remind the instructions at each labeler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taskname = \"SENTENCE_MODIFIED\"\n",
    "project_snli={\n",
    "'title': 'Counterfactual data-augmentation NLI',\n",
    "'description': 'Stanford Natural language Inference',\n",
    "'instructions': '',\n",
    "'input_type': 'TEXT',\n",
    "'json_interface':{\n",
    "    \"jobs\": {\n",
    "        taskname: {\n",
    "            \"mlTask\": \"TRANSCRIPTION\",\n",
    "            \"content\": {\n",
    "                \"input\": None\n",
    "            },\n",
    "            \"required\": 1,\n",
    "            \"isChild\": False,\n",
    "            \"instruction\": \"Write here the modified sentence. Please refer to the instructions above before starting\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_snli['id'] = kili.create_project(title=project_snli['title'],\n",
    "                                                     instructions=project_snli['instructions'],\n",
    "                                                     description=project_snli['description'],\n",
    "                                                     input_type=project_snli['input_type'],\n",
    "                                                     json_interface=project_snli['json_interface'])['id']\n",
    "print(f'Created project {project_snli[\"id\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we'll factorize our code a little, to merge datasets and differentiate properly all the cases of sentences : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_datasets(dataset, sentence_modified) :\n",
    "    url_original = f'https://raw.githubusercontent.com/acmi-lab/counterfactually-augmented-data/master/NLI/original/{dataset}.tsv'\n",
    "    url_revised = f'https://raw.githubusercontent.com/acmi-lab/counterfactually-augmented-data/master/NLI/revised_{sentence_modified}/{dataset}.tsv'\n",
    "    df_original = pd.read_csv(url_original, error_bad_lines=False, sep='\\t')\n",
    "    df_original = df_original[df_original.duplicated(keep='first')== False]\n",
    "    df_original['id'] = df_original.index.astype(str)\n",
    "    \n",
    "    df_revised = pd.read_csv(url_revised, error_bad_lines=False, sep='\\t')\n",
    "    axis_merge = 'sentence2' if sentence_modified=='premise' else 'sentence1'\n",
    "    # keep only one label per set of sentences\n",
    "    df_revised = df_revised[df_revised[[axis_merge,'gold_label']].duplicated(keep='first')== False]\n",
    "\n",
    "    df_merged = df_original.merge(df_revised, how='inner', left_on=axis_merge, right_on=axis_merge)\n",
    "    \n",
    "    if sentence_modified ==  'premise' :\n",
    "        df_merged['Text'] = df_merged['sentence1_x'] + '\\nSENTENCE 2 :\\n' + df_merged['sentence2']\n",
    "        instructions = \" relation, by making a small number of changes in the FIRST SENTENCE\\\n",
    "        such that the document remains coherent and the new label accurately describes the revised passage :\\n\\n\\n\\\n",
    "        SENTENCE 1 :\\n\"\n",
    "    else : \n",
    "        df_merged['Text'] = df_merged['sentence1'] + '\\nSENTENCE 2 :\\n' + df_merged['sentence2_x']\n",
    "        instructions = \" relation, by making a small number of changes in the SECOND SENTENCE\\\n",
    "        such that the document remains coherent and the new label accurately describes the revised passage :\\n\\n\\n\\\n",
    "        SENTENCE 1 : \\n\"\n",
    "    return(df_merged, instructions)\n",
    "\n",
    "def create_external_ids(dataset,dataframe, sentence_modified):\n",
    "    return(('NLI ' + dataset + ' ' + dataframe['gold_label_x'] + ' to ' + dataframe['gold_label_y'] + ' ' + sentence_modified + ' modified ' + dataframe['id']).tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data into Kili\n",
    "We'll add before each set of sentences a small precision of the task for the labeler :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['dev','train','test']\n",
    "sentences_modified = ['premise', 'hypothesis']\n",
    "intro = \"Those two sentences' relation is classified as \"\n",
    "objective = \" to convert to a \"\n",
    "\n",
    "for dataset in datasets :\n",
    "    for sentence_modified in sentences_modified :\n",
    "        df,instructions = merge_datasets(dataset, sentence_modified)\n",
    "\n",
    "        sentences_to_import = create_assets(df, intro, objective, instructions, 'gold_label_x', 'gold_label_y')\n",
    "        external_id_array = create_external_ids(dataset, df, sentence_modified)\n",
    "    \n",
    "        kili.append_many_to_dataset(project_id=project_snli['id'],\n",
    "            content_array=sentences_to_import,\n",
    "            external_id_array=external_id_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the labels into Kili \n",
    "We will fill-in with the results of the study, as if they were predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'results-arxiv:1909.12434'\n",
    "\n",
    "for dataset in datasets :\n",
    "    for sentence_modified in sentences_modified :\n",
    "        axis_changed = 'sentence1_y' if sentence_modified=='premise' else 'sentence2_y'\n",
    "        df,instructions = merge_datasets(dataset, sentence_modified)\n",
    "\n",
    "        external_id_array = create_external_ids(dataset, df, sentence_modified)\n",
    "        json_response_array = create_json_responses(taskname,df,axis_changed) \n",
    "    \n",
    "        kili.create_predictions(project_id=project_snli['id'],\n",
    "            external_id_array=external_id_array,\n",
    "            model_name_array=[model_name]*len(external_id_array),\n",
    "            json_response_array=json_response_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NLI](./img/snli_ex1.png)\n",
    "![NLI](./img/snli_ex2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this tutorial, we learned how Kili can be a great help in your data augmentation task, as it allows to set a simple and easy to use interface, with proper instructions for your task.\n",
    "\n",
    "For the study, the quality of the labeling was a key feature in this complicated task, what Kili allows very simply. To monitor the quality of the results, we could set-up a consensus on a part or all of the annotations, or even keep a part of the dataset as ground truth to measure the performance of every labeler.\n",
    "\n",
    "For an overview of Kili, visit [kili-technology.com](https://kili-technology.com). You can also check out [Kili documentation](https://cloud.kili-technology.com/docs)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
