{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3bd4272-6236-4301-99e5-1a6da27f103a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kili-technology/kili-python-sdk/blob/master/recipes/ner_pre_annotations_openai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51fd409-fad3-48f3-bf19-087db76b43d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Importing OpenAI NER pre-annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671d3a67-0e74-499c-aaff-666808ce46ef",
   "metadata": {},
   "source": [
    "with OpenAI Davinci 3 (?) model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9e89bb-ebb1-4b09-882d-c0083b7ee223",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82a6f46-f9b8-4441-a0f1-a23fe2478a96",
   "metadata": {},
   "source": [
    "Open AI Davinci....  zero-shot ... \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a187fc-6225-4c40-8fdb-cf08e31465f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22df7707-5112-4b01-9877-32097945728c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install kili datasets evaluate ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acf5d41-fc12-48d6-8ae0-199f6f886cf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import json\n",
    "import requests\n",
    "import uuid\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b98a93b-8abc-42a6-9f3f-305f3ff3d78c",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "Hugging Face CONNL dataset => filtering? => Open AI prediction with prompt => kili predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff431f5-a5ca-418b-9d65-27ffae6da631",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f2e97b-c143-4d5e-99d8-1e09ca027c3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84416d2-14a7-4254-a67e-d164de5f63dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset conll2003 (/Users/jonasm/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "308dd78ef3dd4523bd630bec79717a71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"conll2003\", split=\"train\").filter(\n",
    "    lambda datapoint: int(datapoint[\"id\"]) < N\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8318b0dc-471a-4804-b000-95b44d65ff69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "    num_rows: 10\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e0512f-cd9c-4442-a915-8367f790ba5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0', 'tokens': ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7], 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0], 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7575c08b-c8c9-44fb-b72c-7a282d40973e",
   "metadata": {},
   "source": [
    "Here is the meaning of each feature in the dataset:\n",
    "\n",
    "    id: A unique identifier for each token in a sentence.\n",
    "    tokens: The tokens (words or punctuation marks) in a sentence.\n",
    "    pos_tags: Part-of-speech tags for each token in the sentence. Part-of-speech tagging is the process of assigning a tag to each word in a sentence that indicates its part of speech (e.g., noun, verb, adjective, etc.).\n",
    "    chunk_tags: Chunking tags for each token in the sentence. Chunking is the process of grouping words into meaningful phrases based on their syntactic structure.\n",
    "    ner_tags: Named Entity Recognition (NER) tags for each token in the sentence. NER is the task of identifying named entities in text and classifying them into pre-defined categories such as person, organization, location, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febe548d-47da-476d-86d2-5dece489e4bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NER_TAGS_ONTOLOGY = {\n",
    "    \"O\": 0,\n",
    "    \"B-PER\": 1,\n",
    "    \"I-PER\": 2,\n",
    "    \"B-ORG\": 3,\n",
    "    \"I-ORG\": 4,\n",
    "    \"B-LOC\": 5,\n",
    "    \"I-LOC\": 6,\n",
    "    \"B-MISC\": 7,\n",
    "    \"I-MISC\": 8,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9ae135-d4ac-40dd-bb8e-c34e0f143db7",
   "metadata": {},
   "source": [
    "NER_TAGS_ONTOLOGY is a dictionary that maps the named entity tags in the CoNLL2003 dataset to integer labels. Here is the meaning of each key-value pair in the dictionary:\n",
    "\n",
    "    \"O\": 0: Represents the tag \"O\" which means that the token is not part of a named entity.\n",
    "    \"B-PER\": 1: Represents the beginning of a person named entity.\n",
    "    \"I-PER\": 2: Represents a token inside a person named entity.\n",
    "    \"B-ORG\": 3: Represents the beginning of an organization named entity.\n",
    "    \"I-ORG\": 4: Represents a token inside an organization named entity.\n",
    "    \"B-LOC\": 5: Represents the beginning of a location named entity.\n",
    "    \"I-LOC\": 6: Represents a token inside a location named entity.\n",
    "    \"B-MISC\": 7: Represents the beginning of a miscellaneous named entity.\n",
    "    \"I-MISC\": 8: Represents a token inside a miscellaneous named entity.\n",
    "\n",
    "During the training of a Named Entity Recognition model, the entity tags are typically converted to integer labels using a dictionary like NER_TAGS_ONTOLOGY. This allows the model to predict the integer labels during training and inference, instead of predicting the string tags directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a481ef4-cd47-4f4a-8f5c-26ff4ef07871",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Connect with ChatGPT API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d84ab5b-6492-445a-a581-0df8fdcc11a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatGptClient:\n",
    "    def __init__(self, authorization, cookie):\n",
    "        self._headers = {\n",
    "            \"Authorization\": authorization,\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Cookie\": cookie,\n",
    "            \"User-Agent\": (\n",
    "                \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like\"\n",
    "                \" Gecko) Chrome/110.0.0.0 Safari/537.36\"\n",
    "            ),\n",
    "        }\n",
    "        self._conversation_id = None\n",
    "        self._parent_message_id = None\n",
    "\n",
    "    def ask(self, text):\n",
    "        \"\"\"\n",
    "        Send the prompt to ChatGPT.\n",
    "        \"\"\"\n",
    "        question_data = {\n",
    "            \"action\": \"next\",\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"id\": str(uuid.uuid4()),\n",
    "                    \"author\": {\"role\": \"user\"},\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": {\n",
    "                        \"content_type\": \"text\",\n",
    "                        \"parts\": [text],\n",
    "                    },\n",
    "                }\n",
    "            ],\n",
    "            \"parent_message_id\": str(uuid.uuid4())\n",
    "            if self._parent_message_id is None\n",
    "            else self._parent_message_id,\n",
    "            \"model\": \"text-davinci-003\",\n",
    "        }\n",
    "        self._parent_message_id = question_data[\"messages\"][0][\"id\"]\n",
    "        if self._conversation_id is not None:\n",
    "            question_data[\"conversation_id\"] = self._conversation_id\n",
    "        url = \"https://chat.openai.com/backend-api/conversation\"\n",
    "        response = requests.post(url=url, headers=self._headers, json=question_data)\n",
    "        response_parts = [\n",
    "            d\n",
    "            for d in response.content.decode(\"utf-8\").split(\"\\n\")\n",
    "            if '\"content\": {\"content_type\": \"text\", \"parts\": [' in d\n",
    "        ]\n",
    "        response_last_part = response_parts[-1][6:]\n",
    "        response_data = json.loads(response_last_part)\n",
    "        self._conversation_id = response_data[\"conversation_id\"]\n",
    "        return response_data[\"message\"][\"content\"][\"parts\"][0]\n",
    "\n",
    "    def reset_conversation(self):\n",
    "        self._conversation_id = None\n",
    "        self._parent_message_id = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5aa6d1-3a25-4e7f-848c-f79797872c4d",
   "metadata": {},
   "source": [
    "\n",
    "## Evaluation\n",
    "Show that the accuracy of Open AI Da Vinci is good enough to serve as preannotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658b28a9-4a72-49d1-9db3-43632b2bc412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90918134-b3c7-4ce3-ae4f-5d2987b5a981",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aeb80e-a4a8-456f-bc27-e467e5976666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03504b4a-1bd0-4358-8267-3b219ce1f174",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f17966-77e4-4f34-b46b-7e87a7fab890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54a5f75-9f0d-493d-add1-91ba44e6930b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf04fc8-dc08-4445-95cd-752cfbe7517e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cfe9f99-ac11-4213-a375-a402f728a328",
   "metadata": {},
   "source": [
    "model: choose your model: https://platform.openai.com/docs/models/overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc6e882-b518-4866-944c-4182f0ae9cc0",
   "metadata": {},
   "source": [
    "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n",
    "\n",
    "The maximum number of tokens to generate in the completion. The token count of your prompt plus max_tokens cannot exceed the model's context length. Most models have a context length of 2048 tokens (except for the newest models, which support 4096)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496b6f69-17c6-455d-a030-4f4b4161601e",
   "metadata": {},
   "source": [
    "promt: string or array!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0959390-1896-461d-a8c0-4e1a823a7a04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\", prompt=\"Say 'this is a test'\", temperature=0, max_tokens=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2983cd-e828-4efc-897c-d814e1e219c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24929d90-aea8-4e15-bb91-55a87388fdd7",
   "metadata": {},
   "source": [
    "\n",
    "## Import to Kili\n",
    "Import both assets and predictions and show a few screenshots: a good example and a bad example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa81a86c-29a3-40de-9eb8-b3a6b5e427e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if \"KILI_API_KEY\" in os.environ:\n",
    "    KILI_API_KEY = os.environ[\"KILI_API_KEY\"]\n",
    "else:\n",
    "    KILI_API_KEY = getpass.getpass(\"Please enter your Kili API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9339bb-fe2b-49e4-8a0d-412dd4fc20b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kili.client import Kili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8682c4-2eae-49cc-a7d9-758569c06f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonasm/kili-python-sdk/src/kili/authentication.py:44: UserWarning: Kili Python SDK version should match with Kili API version.\n",
      "Please install version: \"pip install kili==2.130.0\"\n",
      "  self.endpoint_kili_version = self.check_versions_match()\n"
     ]
    }
   ],
   "source": [
    "kili = Kili(\n",
    "    api_key=KILI_API_KEY,  # no need to pass the API_KEY if it is already in your environment variables\n",
    "    # api_endpoint=\"https://cloud.kili-technology.com/api/label/v2/graphql\",\n",
    "    # the line above can be uncommented and changed if you are working with an on-premise version of Kili\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af77d734-0fec-4aaf-bbe1-358d78ce7137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40fac8b-6c91-4ceb-8f67-cbe177c62996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f7209b9-7931-4eb3-8408-bb55c77c2453",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "class LabelGPT:\n",
    "    def __init__(self, chatgpt, instructions):\n",
    "        self._chatgpt = chatgpt\n",
    "        self._instructions = instructions\n",
    "\n",
    "    def __get_tokens_index(self, text_tokens, entity_tokens):\n",
    "        index = list(range(len(text_tokens)))\n",
    "        for i, token in enumerate(entity_tokens):\n",
    "            token_index = list(np.where(np.array(text_tokens) == token)[0] - i)\n",
    "            index = list(set(index) & set(token_index))\n",
    "        return index\n",
    "\n",
    "    def ask_iob(self, tokens):\n",
    "        text = \" \".join(tokens)\n",
    "        question = f\"{self._instructions}\\n\\n{text}\"\n",
    "        response = self._chatgpt.ask(question)\n",
    "        self._chatgpt.reset_conversation()\n",
    "        ner_tags = [\"O\"] * len(tokens)\n",
    "        try:\n",
    "            response_json = json.loads(response)\n",
    "        except:\n",
    "            return ner_tags\n",
    "        for key, values in response_json.items():\n",
    "            for value in values:\n",
    "                entity_tokens = value.split(\" \")\n",
    "                entity_index = self.__get_tokens_index(\n",
    "                    text_tokens=tokens,\n",
    "                    entity_tokens=entity_tokens,\n",
    "                )\n",
    "                for i_1 in entity_index:\n",
    "                    for i_2, token in enumerate(entity_tokens):\n",
    "                        prefix = \"B\" if i_2 == 0 else \"I\"\n",
    "                        ner_tags[i_1 + i_2] = prefix + \"-\" + key\n",
    "        return ner_tags\n",
    "\n",
    "\n",
    "def main():\n",
    "    authorization = \"Bearer eyJhbGciOiJSUzI1NiIsIn...\"\n",
    "    cookie = \"__Host-next-auth.csrf-token=45d80225...\"\n",
    "    chatgpt = ChatGPT(authorization, cookie)\n",
    "    instructions = \"\"\"Give, for the following text, the list of:\n",
    "- organisation names\n",
    "- for location names\n",
    "- people names\n",
    "- miscellaneous entity names.\n",
    "The output should be formated as a json with the following keys:\n",
    "- ORG for organisation names\n",
    "- LOC for location names\n",
    "- PER for people names\n",
    "- MISC for miscellaneous.\n",
    "\"\"\"\n",
    "    dataset = load_dataset(\"conll2003\", split=\"train\").filter(\n",
    "        lambda x: int(x[\"id\"]) < 10\n",
    "    )\n",
    "    labelgpt = LabelGPT(chatgpt, instructions)\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    references = []\n",
    "    predictions = []\n",
    "    for d in tqdm(dataset):\n",
    "        tokens = d[\"tokens\"]\n",
    "        iob = labelgpt.ask_iob(tokens)\n",
    "        pprint(list(zip(tokens, iob)))\n",
    "        predictions += [NER_TAGS_ONTOLOGY.get(e, 0) for e in iob]\n",
    "        references += d[\"ner_tags\"]\n",
    "    assert len(predictions) == len(references)\n",
    "    results = accuracy_metric.compute(references=references, predictions=predictions)\n",
    "    print(results)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
