{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kili-technology/kili-python-sdk/blob/main/recipes/ocr_pre_annotations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing OCR pre-annotations\n",
    "\n",
    "In this tutorial we will see how to import OCR pre-annotations in Kili using [Google vision API](https://cloud.google.com/vision/docs/ocr).\n",
    "\n",
    "Pre-annotating your data with OCR will save you a lot of time when annotating transcriptions in Kili.\n",
    "\n",
    "The data that we use comes from [The Street View Text Dataset](http://www.iapr-tc11.org/mediawiki/index.php?title=The_Street_View_Text_Dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading an image from The Street View Dataset in Kili\n",
    "\n",
    "Follow this [link](https://drive.google.com/uc?export=view&id=1ceNwCgLwIyyjPwU42xIoz6mMT3enLewW) to get the image for this tutorial:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ceNwCgLwIyyjPwU42xIoz6mMT3enLewW\" width=\"800\">\n",
    "\n",
    "We will use the Google Vision API to perform Optical Character Recognition on the different inscriptions in this image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the google API, we need to install some packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install google-cloud google-cloud-vision Pillow kili google-cloud-storage wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "\n",
    "from google.cloud import vision\n",
    "from google.oauth2 import service_account\n",
    "from kili.client import Kili\n",
    "from PIL import Image\n",
    "import wget\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the project ontology (json interface).\n",
    "\n",
    "For a transcription task on images, the ontology is a classification job with nested transcriptions for each category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json_interface = {\n",
    "    \"jobs\": {\n",
    "        \"JOB_0\": {  # this job is for annotating the bounding boxes\n",
    "            \"mlTask\": \"OBJECT_DETECTION\",\n",
    "            \"tools\": [\"rectangle\"],\n",
    "            \"instruction\": \"Categories\",\n",
    "            \"required\": 1,\n",
    "            \"isChild\": False,\n",
    "            \"content\": {\n",
    "                \"categories\": {\n",
    "                    \"STORE_INFORMATIONS\": {\"name\": \"Store informations\", \"children\": [\"JOB_1\"]},\n",
    "                    \"PRODUCTS\": {\"name\": \"Products\", \"children\": [\"JOB_2\"]},\n",
    "                },\n",
    "                \"input\": \"radio\",\n",
    "            },\n",
    "        },\n",
    "        \"JOB_1\": {\n",
    "            \"mlTask\": \"TRANSCRIPTION\",\n",
    "            \"instruction\": \"Transcription of store informations\",\n",
    "            \"required\": 1,\n",
    "            \"isChild\": True,\n",
    "        },\n",
    "        \"JOB_2\": {\n",
    "            \"mlTask\": \"TRANSCRIPTION\",\n",
    "            \"instruction\": \"Transcription of products\",\n",
    "            \"required\": 1,\n",
    "            \"isChild\": True,\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the Kili client and create our project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if \"KILI_API_KEY\" not in os.environ:\n",
    "    KILI_API_KEY = getpass.getpass(\"Please enter your API key: \")\n",
    "else:\n",
    "    KILI_API_KEY = os.environ[\"KILI_API_KEY\"]\n",
    "\n",
    "kili = Kili(api_key=KILI_API_KEY)\n",
    "\n",
    "# Create an OCR project\n",
    "project = kili.create_project(\n",
    "    description=\"OCR street view\",\n",
    "    input_type=\"IMAGE\",\n",
    "    json_interface=json_interface,\n",
    "    title=\"Street text annotation\",\n",
    ")\n",
    "project_id = project[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating OCR annotations using Google Vision API\n",
    "\n",
    "We will now see how to perform OCR preannotation on our image using Google Vision API.\n",
    "\n",
    "First you will need to create an account on [Google Cloud](https://cloud.google.com):\n",
    "\n",
    "  1. [create a project](https://console.cloud.google.com/projectcreate) (or use an existing one)\n",
    "  2. then go to the [Cloud Vision API page](https://console.cloud.google.com/apis/library/vision.googleapis.com)\n",
    "  3. activate the API for your project\n",
    "  \n",
    "Now that the API is activated we will need to get a secret key in order to call the API later in our project:\n",
    "\n",
    "  1. go to [API and services page](https://console.cloud.google.com/apis/credentials)\n",
    "  2. and [create a service account](https://console.cloud.google.com/iam-admin/serviceaccounts/create) with authorization to use the vision API\n",
    "  \n",
    "On the [service account details](https://console.cloud.google.com/iam-admin/serviceaccounts) page:\n",
    "\n",
    "  1. click on add a key\n",
    "  2. download the key using json format\n",
    "  3. place the key in your environment variables or in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "google_key_str = os.getenv(\"KILI_API_CLOUD_VISION\")\n",
    "\n",
    "if not google_key_str:\n",
    "    path_to_json_key = \"./google_cloud_key.json\"\n",
    "    with open(path_to_json_key) as file:\n",
    "        google_key_str = file.read()\n",
    "\n",
    "GOOGLE_KEY = json.loads(google_key_str)\n",
    "\n",
    "assert GOOGLE_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start adding OCR pre-annotations to the asset metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def implicit():\n",
    "    from google.cloud import storage\n",
    "\n",
    "    # If you don't specify credentials when constructing the client, the\n",
    "    # client library will look for credentials in the environment.\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # Make an authenticated API request\n",
    "    buckets = list(storage_client.list_buckets())\n",
    "    print(buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def detect_text(path):\n",
    "    \"\"\"Detects text in the file.\"\"\"\n",
    "    credentials = service_account.Credentials.from_service_account_info(GOOGLE_KEY)\n",
    "    client = vision.ImageAnnotatorClient(credentials=credentials)\n",
    "\n",
    "    with io.open(path, \"rb\") as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    response = client.text_detection({\"content\": content})\n",
    "    texts = response._pb.text_annotations\n",
    "    text_annotations = []\n",
    "\n",
    "    for text in texts:\n",
    "        vertices = [{\"x\": vertex.x, \"y\": vertex.y} for vertex in text.bounding_poly.vertices]\n",
    "\n",
    "        tmp = {\n",
    "            \"description\": text.description,\n",
    "            \"boundingPoly\": {\n",
    "                \"vertices\": vertices,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        text_annotations.append(tmp)\n",
    "\n",
    "    if response.error.message:\n",
    "        raise Exception(\n",
    "            \"{}\\nFor more info on error messages, check: \"\n",
    "            \"https://cloud.google.com/apis/design/errors\".format(response.error.message)\n",
    "        )\n",
    "\n",
    "    return text_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH_TO_IMG = wget.download(\n",
    "    \"https://raw.githubusercontent.com/kili-technology/kili-python-sdk/master/recipes/img/store_front.jpeg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 boxes of text.\n",
      "{'description': \"CD\\nITALIAN $1\\nESPRESSO Shot\\nplus tax\\nFINE ITALIAN\\nIMPORTS & DELI\\nJIM\\nIMMY'S FRESH MEATS\\nSAUSAGES\\nFOOD STORE\\nIX\", 'boundingPoly': {'vertices': [{'x': 24, 'y': 6}, {'x': 1668, 'y': 6}, {'x': 1668, 'y': 553}, {'x': 24, 'y': 553}]}}\n"
     ]
    }
   ],
   "source": [
    "text_annotations = detect_text(PATH_TO_IMG)\n",
    "assert len(text_annotations) > 0\n",
    "print(f\"Found {len(text_annotations)} boxes of text.\")\n",
    "print(text_annotations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1680, 1050)\n"
     ]
    }
   ],
   "source": [
    "im = Image.open(PATH_TO_IMG)\n",
    "IMG_WIDTH, IMG_HEIGHT = im.size\n",
    "print(im.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to convert the OCR predictions to the Kili asset metadata format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_text_annotations = {\n",
    "    \"fullTextAnnotation\": {\n",
    "        \"pages\": [{\"height\": IMG_HEIGHT, \"width\": IMG_WIDTH}],\n",
    "    },\n",
    "    \"textAnnotations\": text_annotations,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow the Google Vision API [`AnnotateImageResponse`](https://cloud.google.com/vision/docs/reference/rest/v1/AnnotateImageResponse) format. So in the end, the OCR data to insert into Kili as a JSON metadata contains:\n",
    "\n",
    "- [Full text annotation](https://cloud.google.com/vision/docs/reference/rest/v1/AnnotateImageResponse#TextAnnotation). A list of pages in the document with their respective heights and widths.\n",
    "- [A list of text annotations](https://cloud.google.com/vision/docs/reference/rest/v1/AnnotateImageResponse#EntityAnnotation) with:\n",
    "    - text content\n",
    "    - bounding box coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "  \"fullTextAnnotation\": { \"pages\": [{ \"height\": 914, \"width\": 813 }] },\n",
    "  \"textAnnotations\": [\n",
    "    {\n",
    "      \"description\": \"7SB75\",\n",
    "      \"boundingPoly\": {\n",
    "        \"vertices\": [\n",
    "          { \"x\": 536, \"y\": 259 },\n",
    "          { \"x\": 529, \"y\": 514 },\n",
    "          { \"x\": 449, \"y\": 512 },\n",
    "          { \"x\": 456, \"y\": 257 }\n",
    "        ]\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"description\": \"09TGG\",\n",
    "      \"boundingPoly\": {\n",
    "        \"vertices\": [\n",
    "          { \"x\": 436, \"y\": 256 },\n",
    "          { \"x\": 435, \"y\": 515 },\n",
    "          { \"x\": 360, \"y\": 515 },\n",
    "          { \"x\": 361, \"y\": 256 }\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's upload the asset with its pre-annotations to Kili:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "external_id = \"store\"\n",
    "content = PATH_TO_IMG\n",
    "\n",
    "kili.append_many_to_dataset(\n",
    "    project_id=project_id,\n",
    "    content_array=[content],\n",
    "    external_id_array=[external_id],\n",
    "    json_metadata_array=[full_text_annotations],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "test_cell"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.94it/s]\n"
     ]
    }
   ],
   "source": [
    "assets = kili.assets(project_id=project_id)\n",
    "assert len(assets) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotate in Kili\n",
    "\n",
    "You can now annotate your images and you will see the text automatically extracted:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/kili-technology/kili-python-sdk/master/recipes/img/store_with_ocr.gif\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! 👏\n",
    "\n",
    "Pre-annotating your assets can save you a significant amount of time and improve the accuracy of your labeling ⏳🎯."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "To clean up, we simply need to remove the project that we created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kili.delete_project(project_id)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
