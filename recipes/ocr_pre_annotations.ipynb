{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kili-technology/kili-python-sdk/blob/master/recipes/ocr_pre_annotations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import OCR pre-annotations\n",
    "\n",
    "In this tutorial we will see how to import OCR pre-annotations in Kili using [Google vision API](https://cloud.google.com/vision/docs/ocr).\n",
    "\n",
    "Pre-annotating your data will allow you to gain a significant time when performing OCR using Kili. \n",
    "\n",
    "The data we use comes from [The Street View Text Dataset](http://www.iapr-tc11.org/mediawiki/index.php?title=The_Street_View_Text_Dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading an image from The Street View Dataset in Kili\n",
    "\n",
    "You can obtain the image for this tutorial on this [link](https://drive.google.com/uc?export=view&id=1ceNwCgLwIyyjPwU42xIoz6mMT3enLewW):\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ceNwCgLwIyyjPwU42xIoz6mMT3enLewW\" width=\"800\">\n",
    "\n",
    "We will use the Google to perform an optical caracter recognition of the different texts in the image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the google API, we need to install some packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install google-cloud google-cloud-vision Pillow kili google-cloud-storage wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "\n",
    "from google.cloud import vision\n",
    "from google.oauth2 import service_account\n",
    "from kili.client import Kili\n",
    "from PIL import Image\n",
    "import wget\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the interface we will be using in our project.\n",
    "\n",
    "For OCR, the interface to use is a classification job with nested transcriptions for each category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json_interface = {\n",
    "    \"jobs\": {\n",
    "        \"JOB_0\": {  # this job is for annotating the bounding boxes\n",
    "            \"mlTask\": \"OBJECT_DETECTION\",\n",
    "            \"tools\": [\"rectangle\"],\n",
    "            \"instruction\": \"Categories\",\n",
    "            \"required\": 1,\n",
    "            \"isChild\": False,\n",
    "            \"content\": {\n",
    "                \"categories\": {\n",
    "                    \"STORE_INFORMATIONS\": {\"name\": \"Store informations\", \"children\": [\"JOB_1\"]},\n",
    "                    \"PRODUCTS\": {\"name\": \"Products\", \"children\": [\"JOB_2\"]},\n",
    "                },\n",
    "                \"input\": \"radio\",\n",
    "            },\n",
    "        },\n",
    "        \"JOB_1\": {\n",
    "            \"mlTask\": \"TRANSCRIPTION\",\n",
    "            \"instruction\": \"Transcription of store informations\",\n",
    "            \"required\": 1,\n",
    "            \"isChild\": True,\n",
    "        },\n",
    "        \"JOB_2\": {\n",
    "            \"mlTask\": \"TRANSCRIPTION\",\n",
    "            \"instruction\": \"Transcription of products\",\n",
    "            \"required\": 1,\n",
    "            \"isChild\": True,\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's authenticate to Kili and create our project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if \"KILI_API_KEY\" not in os.environ:\n",
    "    KILI_API_KEY = getpass.getpass(\"Please enter your API key: \")\n",
    "else:\n",
    "    KILI_API_KEY = os.environ[\"KILI_API_KEY\"]\n",
    "\n",
    "kili = Kili(api_key=KILI_API_KEY)\n",
    "\n",
    "# Create an OCR project\n",
    "project = kili.create_project(\n",
    "    description=\"OCR street view\",\n",
    "    input_type=\"IMAGE\",\n",
    "    json_interface=json_interface,\n",
    "    title=\"Street text annotation\",\n",
    ")\n",
    "project_id = project[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating OCR annotations using Google Vision API\n",
    "\n",
    "We will now see how to perform OCR on our image using Google Vision API.\n",
    "\n",
    "First you will need to create an account on https://cloud.google.com:\n",
    "\n",
    "  - [create a project](https://console.cloud.google.com/projectcreate) (or use an existing one)\n",
    "  - then go to the [Cloud Vision API page](https://console.cloud.google.com/apis/library/vision.googleapis.com)\n",
    "  - activate the API for your project (you might need to associate facturation information if you haven't already)\n",
    "  \n",
    "Now that the API is activated we will need to get an API in order to call later in our project:\n",
    "\n",
    "  - go to [API and services page](https://console.cloud.google.com/apis/credentials)\n",
    "  - and [create a service account](https://console.cloud.google.com/iam-admin/serviceaccounts/create) with authorization to use the vision API\n",
    "  \n",
    "On the [service account details](https://console.cloud.google.com/iam-admin/serviceaccounts) page:\n",
    "\n",
    "  - click on add a key\n",
    "  - download the key using json format\n",
    "  - place the key in your environment variables or in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "google_key_str = os.getenv(\"KILI_API_CLOUD_VISION\")\n",
    "\n",
    "if not google_key_str:\n",
    "    path_to_json_key = \"./google_cloud_key.json\"\n",
    "    with open(path_to_json_key) as file:\n",
    "        google_key_str = file.read()\n",
    "\n",
    "GOOGLE_KEY = json.loads(google_key_str)\n",
    "\n",
    "assert GOOGLE_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start to add OCR annotations to the asset metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def implicit():\n",
    "    from google.cloud import storage\n",
    "\n",
    "    # If you don't specify credentials when constructing the client, the\n",
    "    # client library will look for credentials in the environment.\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # Make an authenticated API request\n",
    "    buckets = list(storage_client.list_buckets())\n",
    "    print(buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def detect_text(path):\n",
    "    \"\"\"Detects text in the file.\"\"\"\n",
    "    credentials = service_account.Credentials.from_service_account_info(GOOGLE_KEY)\n",
    "    client = vision.ImageAnnotatorClient(credentials=credentials)\n",
    "\n",
    "    with io.open(path, \"rb\") as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    response = client.text_detection({\"content\": content})\n",
    "    texts = response._pb.text_annotations\n",
    "    text_annotations = []\n",
    "\n",
    "    for text in texts:\n",
    "        vertices = [{\"x\": vertex.x, \"y\": vertex.y} for vertex in text.bounding_poly.vertices]\n",
    "\n",
    "        tmp = {\n",
    "            \"description\": text.description,\n",
    "            \"boundingPoly\": {\n",
    "                \"vertices\": vertices,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        text_annotations.append(tmp)\n",
    "\n",
    "    if response.error.message:\n",
    "        raise Exception(\n",
    "            \"{}\\nFor more info on error messages, check: \"\n",
    "            \"https://cloud.google.com/apis/design/errors\".format(response.error.message)\n",
    "        )\n",
    "\n",
    "    return text_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH_TO_IMG = wget.download(\n",
    "    \"https://raw.githubusercontent.com/kili-technology/kili-python-sdk/master/recipes/img/store_front.jpeg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 boxes of text.\n",
      "{'description': \"CD\\nITALIAN $1\\nESPRESSO Shot\\nplus tax\\nFINE ITALIAN\\nIMPORTS & DELI\\nJIM\\nIMMY'S FRESH MEATS\\nSAUSAGES\\nFOOD STORE\\nIX\", 'boundingPoly': {'vertices': [{'x': 24, 'y': 6}, {'x': 1668, 'y': 6}, {'x': 1668, 'y': 553}, {'x': 24, 'y': 553}]}}\n"
     ]
    }
   ],
   "source": [
    "text_annotations = detect_text(PATH_TO_IMG)\n",
    "assert len(text_annotations) > 0\n",
    "print(f\"Found {len(text_annotations)} boxes of text.\")\n",
    "print(text_annotations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1680, 1050)\n"
     ]
    }
   ],
   "source": [
    "im = Image.open(PATH_TO_IMG)\n",
    "IMG_WIDTH, IMG_HEIGHT = im.size\n",
    "print(im.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to convert the OCR predictions to the Kili asset metadata format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_text_annotations = {\n",
    "    \"fullTextAnnotation\": {\n",
    "        \"pages\": [{\"height\": IMG_HEIGHT, \"width\": IMG_WIDTH}],\n",
    "    },\n",
    "    \"textAnnotations\": text_annotations,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow the Google Vision API [`AnnotateImageResponse`](https://cloud.google.com/vision/docs/reference/rest/v1/AnnotateImageResponse) format. So in the end, the OCR data to insert into Kili as a JSON metadata contains:\n",
    "\n",
    "- [Full text annotation](https://cloud.google.com/vision/docs/reference/rest/v1/AnnotateImageResponse#TextAnnotation). A list of pages in the document with their respective heights and widths.\n",
    "- [A list of text annotations](https://cloud.google.com/vision/docs/reference/rest/v1/AnnotateImageResponse#EntityAnnotation) with:\n",
    "    - the text content;\n",
    "    - coordinates of the bounding box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "  \"fullTextAnnotation\": { \"pages\": [{ \"height\": 914, \"width\": 813 }] },\n",
    "  \"textAnnotations\": [\n",
    "    {\n",
    "      \"description\": \"7SB75\",\n",
    "      \"boundingPoly\": {\n",
    "        \"vertices\": [\n",
    "          { \"x\": 536, \"y\": 259 },\n",
    "          { \"x\": 529, \"y\": 514 },\n",
    "          { \"x\": 449, \"y\": 512 },\n",
    "          { \"x\": 456, \"y\": 257 }\n",
    "        ]\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"description\": \"09TGG\",\n",
    "      \"boundingPoly\": {\n",
    "        \"vertices\": [\n",
    "          { \"x\": 436, \"y\": 256 },\n",
    "          { \"x\": 435, \"y\": 515 },\n",
    "          { \"x\": 360, \"y\": 515 },\n",
    "          { \"x\": 361, \"y\": 256 }\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's upload the asset with its pre-annotations to Kili:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "external_id = \"store\"\n",
    "content = PATH_TO_IMG\n",
    "\n",
    "kili.append_many_to_dataset(\n",
    "    project_id=project_id,\n",
    "    content_array=[content],\n",
    "    external_id_array=[external_id],\n",
    "    json_metadata_array=[full_text_annotations],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "test_cell"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.94it/s]\n"
     ]
    }
   ],
   "source": [
    "assets = kili.assets(project_id=project_id)\n",
    "assert len(assets) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotate in Kili\n",
    "\n",
    "You can now annotate your images and you will see the text automatically extracted:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/kili-technology/kili-python-sdk/master/recipes/img/store_with_ocr.gif\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! üëè\n",
    "\n",
    "Pre-annotating your assets can save you a significant amount of time and improve the accuracy of your labeling ‚è≥üéØ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "To clean up, we simply need to remove the project that we created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kili.delete_project(project_id)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
