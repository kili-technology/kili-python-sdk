{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kili Tutorial: Importing inference labels\n",
    "\n",
    "In this tutorial, we will walk through the process of using Kili to evaluate the performance of a machine learning model in production. The goal of this tutorial is to illustrate how to push such labels, and how to visualize the quality of those predicted labels.\n",
    "\n",
    "Additionally:\n",
    "\n",
    "For an overview of Kili, visit https://kili-technology.com. You can also check out the Kili documentation https://docs.kili-technology.com/docs.\n",
    "\n",
    "The tutorial is divided into two parts:\n",
    "\n",
    "1. Giving a bit of context\n",
    "2. How to make use of inference labels in practice\n",
    "\n",
    "This next cell connects the notebook to the Kili API. You need to update the credentials `api_key` before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#!pip install kili\n",
    "from kili.client import Kili\n",
    "\n",
    "api_endpoint = os.getenv(\"KILI_API_ENDPOINT\")\n",
    "# If you use Kili SaaS, use the url 'https://cloud.kili-technology.com/api/label/v2/graphql'\n",
    "\n",
    "kili = Kili(api_endpoint=api_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you have a trained machine learning model $m$, which can, given a data $x$, output a prediction (ie, an inference label) $l^i = m(x)$.\n",
    "\n",
    "What you will probably want to do is monitor the quality of such predictions, as the model evolves. Kili allows you to better monitor and iterate on your model, thanks to the concept of agreement. An agreement is a quantitative measure of similarity between two different labels. In Kili, there are three main features derived from agreement : \n",
    "\n",
    "- [Consensus](https://docs.kili-technology.com/docs/consensus-overview), which is the agreement between two labelers.\n",
    "- [Honeypot](https://docs.kili-technology.com/docs/honeypot-overview) which is the agreement between a \"super human annotator\" and a labeler.\n",
    "- **Inference**, which is the agreement between a machine learning inference label and a human.\n",
    "\n",
    "Those number can be monitored from the [queue page](https://docs.kili-technology.com/docs/queue-page) or the [analytics page](https://docs.kili-technology.com/docs/analytics-page). You can find how the agreement is computed [here](https://docs.kili-technology.com/docs/calculation-rules-for-quality-metrics)\n",
    "\n",
    "In this tutorial, we will put an emphasis on **Inference**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Use cases "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We identify two main use cases for the use of **inference** :\n",
    "\n",
    "1. **You have a model in production**. When it receives assets, it automatically feeds a Kili project with both the asset and the predicted label. **You also have human workforce, whose job is to monitor the quality of the model**. They regularly manually label some data seen by the model.\n",
    "    - When a human submits a label, the inference score for that label is automatically computed using the predicted label.\n",
    "    - Low inference scores can indicate either a model performing badly on some kind of data, or a disagreement between humans and the model. This can help you to :\n",
    "    \n",
    "        - `Detect data drift`\n",
    "        - `Identify data on which the model needs improvement`\n",
    "       \n",
    "       \n",
    "2. **You used Kili to label data**, and you have **the first iteration of your model**. You can use **a part of the dataset as testing data**, and quickly get **test scores**. You could of course use your own metrics (rather than our own definition of agreement), but using Kili allows you to quickly filter and indentify the assets where your model is most different from the ground truth.\n",
    "    - When you push an inference label on an asset, the inference score is automatically computed for all most recent labels of the different people who labeled this asset.\n",
    "    - You can filter on low inference score, to understand why your model is failing, and how to fix it (getting more data, splitting or merging categories, etc...)\n",
    "\n",
    "\n",
    "Using Kili for monitoring or developing your model allows you to quickly iterate on the data used to train your model, allowing to get a better model faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - In practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Use case 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating a project and defining a model which, given an asset input x, returns a category (random in our example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_interface = {\n",
    "    \"jobs\": {\n",
    "        \"CLASSIFICATION_JOB\": {\n",
    "            \"mlTask\": \"CLASSIFICATION\",\n",
    "            \"content\": {\n",
    "                \"categories\": {\n",
    "                    \"RED\": {\"name\": \"Red\"},\n",
    "                    \"BLACK\": {\"name\": \"Black\"},\n",
    "                    \"WHITE\": {\"name\": \"White\"},\n",
    "                    \"GREY\": {\"name\": \"Grey\"},\n",
    "                },\n",
    "                \"input\": \"radio\",\n",
    "            },\n",
    "            \"required\": 0,\n",
    "            \"isChild\": False,\n",
    "            \"instruction\": \"Color\",\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "project_id = kili.create_project(\n",
    "    title=\"Project demo inference\", input_type=\"IMAGE\", json_interface=json_interface\n",
    ")[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can simulate that our model is in production. Each time it receives an asset, it pushes it as well as the label it predicted to the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_of_assets = [\n",
    "    {\n",
    "        \"url\": (\n",
    "            \"https://storage.googleapis.com/label-public-staging/recipes/inference/black_car.jpg\"\n",
    "        ),\n",
    "        \"external_id\": \"black_car.jpg\",\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://storage.googleapis.com/label-public-staging/recipes/inference/grey_car.jpg\",\n",
    "        \"external_id\": \"grey_car.jpg\",\n",
    "    },\n",
    "    {\n",
    "        \"url\": (\n",
    "            \"https://storage.googleapis.com/label-public-staging/recipes/inference/white_car.jpg\"\n",
    "        ),\n",
    "        \"external_id\": \"white_car.jpg\",\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://storage.googleapis.com/label-public-staging/recipes/inference/red_car.jpg\",\n",
    "        \"external_id\": \"red_car.jpg\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {\n",
    "    \"black_car.jpg\": \"WHITE\",\n",
    "    \"grey_car.jpg\": \"GREY\",\n",
    "    \"white_car.jpg\": \"RED\",\n",
    "    \"red_car.jpg\": \"BLACK\",\n",
    "}\n",
    "\n",
    "kili.append_many_to_dataset(\n",
    "    project_id=project_id,\n",
    "    content_array=[asset[\"url\"] for asset in stream_of_assets],\n",
    "    external_id_array=[asset[\"external_id\"] for asset in stream_of_assets],\n",
    ")\n",
    "assets = list(kili.assets(project_id=project_id, fields=[\"id\", \"externalId\"], disable_tqdm=True))\n",
    "assets_ids = [asset[\"id\"] for asset in assets]\n",
    "predicted_categories = [predictions[asset[\"externalId\"]] for asset in assets]\n",
    "inference_labels = [\n",
    "    {\"CLASSIFICATION_JOB\": {\"categories\": [{\"name\": predicted_category}]}}\n",
    "    for predicted_category in predicted_categories\n",
    "]\n",
    "kili.append_labels(\n",
    "    json_response_array=inference_labels,\n",
    "    asset_id_array=assets_ids,\n",
    "    label_type=\"INFERENCE\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, human labelers can annotate a subsample of the assets pushed to Kili. \n",
    "\n",
    "Note : you can even automatically [prioritize assets](https://docs.kili-technology.com/docs/queue-prioritization) to be reviewed by a human by using the model's uncertainty. When the model is unsure of its predictions, it may indicate wrong labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truths = {\n",
    "    \"black_car.jpg\": \"BLACK\",\n",
    "    \"grey_car.jpg\": \"GREY\",\n",
    "    \"white_car.jpg\": \"WHITE\",\n",
    "    \"red_car.jpg\": \"RED\",\n",
    "}\n",
    "\n",
    "human_labels = [\n",
    "    {\"CLASSIFICATION_JOB\": {\"categories\": [{\"name\": ground_truths[asset[\"externalId\"]]}]}}\n",
    "    for asset in assets\n",
    "]\n",
    "kili.append_labels(\n",
    "    json_response_array=human_labels,\n",
    "    asset_id_array=assets_ids,\n",
    "    label_type=\"DEFAULT\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now fetch the agreement between the human and the model, for human labels :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kili.labels(\n",
    "    project_id=project_id, fields=[\"inferenceMark\", \"id\", \"labelOf.id\"], type_in=[\"DEFAULT\"]\n",
    ")\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing\n",
    "\n",
    "for label in labels:\n",
    "    external_id = [\n",
    "        asset[\"externalId\"] for asset in assets if asset[\"id\"] == label[\"labelOf\"][\"id\"]\n",
    "    ][0]\n",
    "    if predictions[external_id] == ground_truths[external_id]:\n",
    "        assert label[\"inferenceMark\"] == 1\n",
    "    else:\n",
    "        assert label[\"inferenceMark\"] < 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows you to identify problems :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in labels:\n",
    "    if label[\"inferenceMark\"] < 1:\n",
    "        inference_label = kili.labels(\n",
    "            project_id=project_id,\n",
    "            asset_id=label[\"labelOf\"][\"id\"],\n",
    "            type_in=[\"INFERENCE\"],\n",
    "            disable_tqdm=True,\n",
    "        )[0]\n",
    "        human_label = kili.labels(project_id=project_id, label_id=label[\"id\"], disable_tqdm=True)[0]\n",
    "        inference_category = inference_label[\"jsonResponse\"][\"CLASSIFICATION_JOB\"][\"categories\"][0][\n",
    "            \"name\"\n",
    "        ]\n",
    "        human_category = human_label[\"jsonResponse\"][\"CLASSIFICATION_JOB\"][\"categories\"][0][\"name\"]\n",
    "        print(f\"The model predicted {inference_category} but the human predicted {human_category}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also find the assets with most disagreement directly from the interface with the filter \"Human/Model IOU\". Low IOU indicates low agreement : \n",
    "\n",
    "![inference](https://storage.googleapis.com/label-public-staging/recipes/inference/inference_filter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Use case 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can invert the previous use case. We start by having a human labeled dataset, and we insert model predictions, to simulate testing our model on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = kili.create_project(\n",
    "    title=\"Project demo inference 2\", input_type=\"IMAGE\", json_interface=json_interface\n",
    ")[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_assets = [\n",
    "    {\n",
    "        \"url\": (\n",
    "            \"https://storage.googleapis.com/label-public-staging/recipes/inference/black_car.jpg\"\n",
    "        ),\n",
    "        \"external_id\": \"black_car.jpg\",\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://storage.googleapis.com/label-public-staging/recipes/inference/grey_car.jpg\",\n",
    "        \"external_id\": \"grey_car.jpg\",\n",
    "    },\n",
    "    {\n",
    "        \"url\": (\n",
    "            \"https://storage.googleapis.com/label-public-staging/recipes/inference/white_car.jpg\"\n",
    "        ),\n",
    "        \"external_id\": \"white_car.jpg\",\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://storage.googleapis.com/label-public-staging/recipes/inference/red_car.jpg\",\n",
    "        \"external_id\": \"red_car.jpg\",\n",
    "    },\n",
    "]\n",
    "ground_truths = {\n",
    "    \"black_car.jpg\": \"BLACK\",\n",
    "    \"grey_car.jpg\": \"GREY\",\n",
    "    \"white_car.jpg\": \"WHITE\",\n",
    "    \"red_car.jpg\": \"RED\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kili.append_many_to_dataset(\n",
    "    project_id=project_id,\n",
    "    content_array=[asset[\"url\"] for asset in labeled_assets],\n",
    "    external_id_array=[asset[\"external_id\"] for asset in labeled_assets],\n",
    ")\n",
    "assets = list(kili.assets(project_id=project_id, fields=[\"id\", \"externalId\"]))\n",
    "assets_ids = [asset[\"id\"] for asset in assets]\n",
    "human_labels = [\n",
    "    {\"CLASSIFICATION_JOB\": {\"categories\": [{\"name\": ground_truths[asset[\"externalId\"]]}]}}\n",
    "    for asset in assets\n",
    "]\n",
    "kili.append_labels(\n",
    "    json_response_array=human_labels,\n",
    "    asset_id_array=assets_ids,\n",
    "    label_type=\"DEFAULT\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then our model is fit using maybe 80% of the training data. We can then run it against the remaining 20%, and upload its predictions to Kili :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = [\n",
    "    {\"CLASSIFICATION_JOB\": {\"categories\": [{\"name\": predictions[asset[\"externalId\"]]}]}}\n",
    "    for asset in assets\n",
    "]\n",
    "kili.append_labels(\n",
    "    json_response_array=test_labels,\n",
    "    asset_id_array=assets_ids,\n",
    "    label_type=\"INFERENCE\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kili.labels(\n",
    "    project_id=project_id, fields=[\"inferenceMark\", \"id\", \"labelOf.id\"], type_in=[\"DEFAULT\"]\n",
    ")\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing\n",
    "for label in labels:\n",
    "    external_id = [\n",
    "        asset[\"externalId\"] for asset in assets if asset[\"id\"] == label[\"labelOf\"][\"id\"]\n",
    "    ][0]\n",
    "    if predictions[external_id] == ground_truths[external_id]:\n",
    "        assert label[\"inferenceMark\"] == 1\n",
    "    else:\n",
    "        assert label[\"inferenceMark\"] < 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in labels:\n",
    "    if label[\"inferenceMark\"] < 1:\n",
    "        inference_label = list(\n",
    "            kili.labels(\n",
    "                project_id=project_id,\n",
    "                asset_id=label[\"labelOf\"][\"id\"],\n",
    "                type_in=[\"INFERENCE\"],\n",
    "                disable_tqdm=True,\n",
    "            )\n",
    "        )[0]\n",
    "        human_label = list(\n",
    "            kili.labels(project_id=project_id, label_id=label[\"id\"], disable_tqdm=True)\n",
    "        )[0]\n",
    "        inference_category = inference_label[\"jsonResponse\"][\"CLASSIFICATION_JOB\"][\"categories\"][0][\n",
    "            \"name\"\n",
    "        ]\n",
    "        human_category = human_label[\"jsonResponse\"][\"CLASSIFICATION_JOB\"][\"categories\"][0][\"name\"]\n",
    "        print(f\"The human predicted {human_category} but the model predicted {inference_category}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also find the assets where the prediction and the human disagree most directly from the interface with the filter \"Human/Model IOU\" : \n",
    "\n",
    "![inference](https://storage.googleapis.com/label-public-staging/recipes/inference/inference_test_filter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we accomplished the following:\n",
    "\n",
    "We introduced the concept of Kili inference labels. We showed how to make use of such labels, in two practical use cases.\n",
    "\n",
    "You can also visit the Kili website or Kili documentation for more info!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
