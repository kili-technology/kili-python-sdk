{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Banner Image -->\n",
    "<center>\n",
    "<img src=\"https://a.storyblok.com/f/139616/x/17b1d8406f/kili-icon-dark-mode.svg\" width=\"10%\">\n",
    "\n",
    "  <a href=\"https://kili-technology.com/\" style=\"color: #06b6d4;\">Kili website</a> --\n",
    "  <a href=\"https://cloud.kili-technology.com/label\" style=\"color: #06b6d4;\">Log-in</a> --\n",
    "  <a href=\"https://docs.kili-technology.com/docs\" style=\"color: #06b6d4;\">Docs</a>\n",
    "\n",
    "\n",
    "\n",
    "# Kili - Fine-tune LLAMA2 on your own data\n",
    "</center>\n",
    "\n",
    "Welcome!\n",
    "\n",
    "In this notebook, we will fine-tune the [7B version of LLAMA2](https://huggingface.co/meta-llama/Llama-2-7b) from Meta leveraging QLoRA.\n",
    "To do so, we will rely on the `transformers` library and `bitsandbytes`to load model in 4bit, `PEFT` and `trl`for model training using LoRA.\n",
    "\n",
    "First, we'll load an off-the-shelf financial dataset and prepare the data. Then, we'll test how well the original model performs on our dataset. After that, we'll launch our training to fine-tune it. Finally, we'll be able to test our fine-tuned model and compare it to the original model.\n",
    "\n",
    "Feel free to adapt with a different dataset to create your custom model!\n",
    "\n",
    "\n",
    "## 0. Pre-requisites\n",
    "\n",
    "In addition to package loading, you need to be connected to your Hugging Face space to download the model (and upload your fine-tuned model if you want to). Make sure that you have access rights to [Meta's HF space](https://huggingface.co/meta-llama/Llama-2-7b-hf) to be able to download the model later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIrst, let's download and prepare all the tools that we'll need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q accelerate==0.21.0 peft==0.5.0 bitsandbytes==0.41.1 transformers==4.34.0 trl==0.7.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    logging,\n",
    "    pipeline,\n",
    ")\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging face token will be required to download & upload dataset & models\n",
    "HF_TOKEN = getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data\n",
    "\n",
    "Data is in that case loaded from an existing Hugging Face dataset (see [AdaptLLM space](https://huggingface.co/datasets/AdaptLLM/finance-tasks)).\n",
    "\n",
    "You can use a different dataset, or even load your own data to fine-tune the model with it. You can start with several hundred examples.\n",
    "\n",
    "If you need to create the correct input for such fine-tuning, this is where [Kili](https://kili-technology.com/) can help with our state-of-the-art labeling interfaces and quality workflows. Note that if you don't have the time or sufficient resources, Kili also offers professional end-to-end labeling services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset characteristics\n",
    "# We select the ConvFinQA dataset from the finance-tasks space. This dataset only has a test split - See https://huggingface.co/datasets/AdaptLLM/finance-tasks\n",
    "dataset_name = \"AdaptLLM/finance-tasks\"\n",
    "subset = \"ConvFinQA\"\n",
    "split = \"test\"\n",
    "\n",
    "# Dataset loading\n",
    "raw_dataset = load_dataset(dataset_name, subset, split=split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format the prompt\n",
    "\n",
    "Let's create a function to format the raw data.\n",
    "\n",
    "In our case, our dataset has an input and a label that we merge into a single piece of text for the training.\n",
    "\n",
    "If you use a different dataset, you'll have to adapt this function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting(dataset):\n",
    "    dataset[\"input+labels\"] = f\"### Question: {dataset['input']}\\n ### Answer: {dataset['label']}\"\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply our formating to the raw dataset and split it  with  `train_test_split` to keep a couple of examples for later trials.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = raw_dataset.map(formatting).train_test_split(test_size=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load base model\n",
    "\n",
    "Before loading the base model from the meta-llama repository, make sure that you have access rights (see in [Meta's HF space](https://huggingface.co/meta-llama/Llama-2-7b-hf))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model name in HF\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = torch.float16\n",
    "\n",
    "# Configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Activate 4-bit\n",
    "    bnb_4bit_use_double_quant=False,  # double quantization for 4-bit base models\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Quantization type (fp4 or nf4)\n",
    "    bnb_4bit_compute_dtype=compute_dtype,  # Compute dtype for 4-bit base models\n",
    ")\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, quantization_config=bnb_config, token=HF_TOKEN\n",
    ")\n",
    "\n",
    "# Key values are disregarded for the fine-tuned model.\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenization\n",
    "\n",
    "Let's load our tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, padding_side=\"right\", add_eos_token=True, add_bos_token=True, token=HF_TOKEN\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the tokenizer to be able to understand the length of the items and adapt as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt(dataset):\n",
    "    result = tokenizer(dataset[\"input+labels\"])\n",
    "    return result\n",
    "\n",
    "\n",
    "tokenized_train_dataset = dataset[\"train\"].map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean, median\n",
    "\n",
    "lengths = [len(x[\"input_ids\"]) for x in tokenized_train_dataset]\n",
    "\n",
    "max_length = max(lengths)\n",
    "\n",
    "print(f\"Mean:{mean(lengths)}, Median: {median(lengths)}, Max:{max(lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the token is too long, our computing resources won't be able to process it. We'll remove these items from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 1250\n",
    "\n",
    "# Use the map function to filter out items below the threshold length\n",
    "filtered_dataset = tokenized_train_dataset.filter(lambda item: len(item[\"input_ids\"]) <= max_length)\n",
    "\n",
    "# Display the filtered dataset\n",
    "print(filtered_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check lengths of our filtered dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(x[\"input_ids\"]) for x in filtered_dataset]\n",
    "\n",
    "print(f\"Mean:{mean(lengths)}, Median: {median(lengths)}, Max:{max(lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create two datasets, so that we have some data to later evaluate the model. Also: we'll remove columns that are not required from the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = filtered_dataset.map(\n",
    "    remove_columns=[\"id\", \"input\", \"label\", \"input_ids\", \"attention_mask\"]\n",
    ")\n",
    "val_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test base model\n",
    "\n",
    "Let's check how an off-the-shelf Llama 2 7B does on one of our data samples with the following `prompt_eval`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt = \"\"\"\n",
    "### Question: Given the following data\n",
    "cash flowsmillions | 2014 | 2013 | 2012\n",
    "cash provided by operating activities | $7385 | $6823 | $6161\n",
    "cash used in investing activities | -4249 (4249) | -3405 (3405) | -3633 (3633)\n",
    "cash used in financing activities | -2982 (2982) | -3049 (3049) | -2682 (2682)\n",
    "net change in cash and cashequivalents | $154 | $369 | $-154 (154)\n",
    "\n",
    "what was the net change in cash and cashequivalents for 2013?\n",
    "\n",
    "### Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Run text generation pipeline with our next model\n",
    "prompt = eval_prompt\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=15)\n",
    "result = pipe(prompt)\n",
    "print(result[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training\n",
    "\n",
    "*Optional* - You can use Weights & Biases for experiment tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q wandb -U\n",
    "\n",
    "import os\n",
    "\n",
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = \"finance-finetune\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train our model.\n",
    "\n",
    "The parameters below have been set based on the standard configuration, but feel free to adapt them based on your requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=0,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"wandb\",\n",
    "    gradient_checkpointing=True,  ## Required since the introduction of update to prepare_model_for_kbit_training\n",
    ")\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"input+labels\",\n",
    "    max_seq_length=max_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuned model name\n",
    "new_model = \"Llama-2-7b-hf-finance-v01\"\n",
    "\n",
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.Try the Trained Model!\n",
    "\n",
    "The PEFT library only saves the QLoRA adapters, so the initial Llama2 needs to be loaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,  # Llama 2 7B, same as before\n",
    "    quantization_config=bnb_config,  # Same quantization config as before\n",
    "    device_map={\"\": 0},\n",
    "    trust_remote_code=True,\n",
    "    token=HF_TOKEN,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# Associate with QLoRA adaptaters of the new_model\n",
    "model = PeftModel.from_pretrained(base_model, new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"right\", trust_remote_code=True)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use our pipeline to test our new model.\n",
    "\n",
    "Let's try our new model on one of the elements that we left in our validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_item = val_dataset[0]\n",
    "\n",
    "prompt = f\"\"\"### Question: {val_item[\"input\"]}\n",
    "\"\"\"\n",
    "\n",
    "print(val_item[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 15\n",
    "\n",
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Run text generation pipeline with our next model\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=max_tokens)\n",
    "result = pipe(prompt)\n",
    "print(result[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare with the original model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,  # Llama 2 7B, same as before\n",
    "    quantization_config=bnb_config,  # Same quantization config as before\n",
    "    device_map={\"\": 0},\n",
    "    trust_remote_code=True,\n",
    "    token=HF_TOKEN,\n",
    "    low_cpu_mem_usage=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run text generation pipeline with our next model\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\", model=original_model, tokenizer=tokenizer, max_new_tokens=max_tokens\n",
    ")\n",
    "result = pipe(prompt)\n",
    "print(result[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Push to HF\n",
    "\n",
    "*Optional* - if you want to save the adaptaters of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(new_model, use_temp_dir=False, token=HF_TOKEN)\n",
    "tokenizer.push_to_hub(new_model, use_temp_dir=False, token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "With this notebook we have adapted our generic LLAMA2 model to a finance dataset for domain adaptation & specialization purposes.\n",
    "We can assess that with a few hundred examples, the fine-tuned model has been able to structure its answers according to the fine-tuning dataset format, and also better identify the answer to the financial question.\n",
    "\n",
    "Next step would be to run a larger scale evaluation of our fine-tuned model so as to assess its actual performance improvement on a financial Q&A task. Such task can be done by creating a benchmark dataset and evaluating both the initial model and the fine-tuned model on this same dataset. [Kili](https://kili-technology.com/) can provide support in this step with both the software and the service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Help us improve this tutorial by providing feedback ðŸ˜€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Banner Image -->\n",
    "<center>\n",
    "<img src=\"https://a.storyblok.com/f/139616/x/17b1d8406f/kili-icon-dark-mode.svg\" width=\"10%\">\n",
    "\n",
    "  <a href=\"https://kili-technology.com/\" style=\"color: #06b6d4;\">Kili website</a> --\n",
    "  <a href=\"https://cloud.kili-technology.com/label\" style=\"color: #06b6d4;\">Log-in</a> --\n",
    "  <a href=\"https://docs.kili-technology.com/docs\" style=\"color: #06b6d4;\">Docs</a>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
